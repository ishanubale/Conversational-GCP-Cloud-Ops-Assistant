ðŸ¤– CloudOps AI Assistant (LangChain + Vertex AI) 
   This project is converstion AI CloudOps Assistant built using LangChain, Google Cloud Vertex AI (Gemini models + Embeddings), and Streamlit for userinterface. It is designed to troubleshoot GCP issues, provide step-by-step guidance, and leverage Retrieval-Augmented Generation (RAG) for real-time documentation support.

ðŸš€ Features 
- LLM Integration (Vertex AI) â†’ Uses gemini-2.5-flash-lite for low-latency troubleshooting. 
- RAG   (Retrieval-Augmented Generation) â†’ Fetches and indexes GCP documentation for context-aware answers. 
- FAISS Vector Store â†’ Stores embeddings for fast document retrieval. 
- Secure Config â†’ Environment variables & service account authentication. 
- Streamlit Frontend â†’ Simple chat UI for back-and-forth conversations.

ðŸ“‚ Project Structure 
 â”œâ”€â”€ agent.py # Main agent logic (LLM + RAG) 
 â”œâ”€â”€ UI.py # Streamlit chat UI 
 â”œâ”€â”€ ragInit.py # RAG initialization (load docs, create embeddings) 
 â”œâ”€â”€ .env # Environment variables 
 â”œâ”€â”€ requirements.txt # Python dependencies 
 â”œâ”€â”€ .gitignore # Git ignore rules 
 â””â”€â”€ gcp_faiss_index # (Ignored) Vector embeddings storage created using script

 Steps for setup: 
 1. Clone the Repository 
 2. Create Virtual Environment 
 3. Install Dependencies pip install -r requirements.txt 
 4. Setup Google Cloud Authentication Enable Vertex AI API in your GCP project. Create a Service Account with     Vertex AI User and Storage Object Viewer roles. Download the JSON key and store it in your project (e.g., serviceKey.json). 
 5.Configure Environment Variables GOOGLE_APPLICATION_CREDENTIALS, GOOGLE_CLOUD_PROJECT, GOOGLE_CLOUD_REGION.

 Steps to run: 
 1. Initialize RAG = cmd-> python ragInit.py 
 2. Run Agent (CLI Test) = cmd -> python agent.py 
 3. Start Chat UI = cmd -> streamlit run UI.py
